#!/bin/bash
# WSL2 Ollama å¯åŠ¨è„šæœ¬

echo "ğŸš€ Starting Ollama on port 11435..."

# æ£€æŸ¥11435ç«¯å£æ˜¯å¦å¯ç”¨
if sudo lsof -i :11435 > /dev/null 2>&1; then
    echo "âš ï¸  Port 11435 is already in use"
    sudo lsof -i :11435
    exit 1
fi

# å¯åŠ¨Ollama
echo "ğŸ¤– Starting Ollama server..."
OLLAMA_HOST=0.0.0.0:11435 ollama serve &

# ç­‰å¾…æœåŠ¡å¯åŠ¨
echo "â³ Waiting for Ollama to start..."
sleep 5

# æµ‹è¯•è¿æ¥
echo "ğŸ” Testing connection..."
if curl -s http://localhost:11435/api/version > /dev/null; then
    echo "âœ… Ollama is running on port 11435"
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹
    echo "ğŸ“š Checking available models..."
    ollama list
    
    echo ""
    echo "ğŸ’¡ To download a model (if none available):"
    echo "   OLLAMA_HOST=localhost:11435 ollama pull llama3.2:3b"
    echo ""
    echo "ğŸ¯ To start your project:"
    echo "   export OLLAMA_PORT=11435"
    echo "   npm run server"
    
else
    echo "âŒ Failed to start Ollama"
    exit 1
fi
