#!/bin/bash
# é¡¹ç›®å¯åŠ¨è„šæœ¬ - ä½¿ç”¨WSL2 Ollama

echo "ğŸ½ï¸  Starting Meal Dice with WSL2 Ollama..."

# è®¾ç½®ç¯å¢ƒå˜é‡
export OLLAMA_PORT=11435

# æ£€æŸ¥Ollamaæ˜¯å¦åœ¨11435ç«¯å£è¿è¡Œ
if ! curl -s http://localhost:11435/api/version > /dev/null; then
    echo "âŒ Ollama is not running on port 11435"
    echo "ğŸ’¡ Please run: ./start-ollama-wsl2.sh first"
    exit 1
fi

echo "âœ… Ollama detected on port 11435"

# æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹
echo "ğŸ” Checking for available models..."
model_count=$(curl -s http://localhost:11435/api/tags | grep -o '"models":\[' | wc -l)

if [ "$model_count" -eq 0 ]; then
    echo "âš ï¸  No models found. Downloading llama3.2:3b..."
    OLLAMA_HOST=localhost:11435 ollama pull llama3.2:3b
fi

echo "ğŸš€ Starting Node.js server..."
npm run server
